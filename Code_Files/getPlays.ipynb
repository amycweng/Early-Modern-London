{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extractTexts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the entire body text from an XML document.  \n",
    "'''\n",
    "# ids for the plays in Sarah's spreadsheet \n",
    "tcpIDs = ['A03208','A03197','A20100','A07493','A04633','A11153','A18407',\n",
    "        'A19260','A07524','A07505','A04648','A11146','A20083','A20098',\n",
    "        'A07065','A07025','A02092','A03224','A12078','A18427','A12142',\n",
    "        'A77565','A77567','A53070','A46228','A59985','A04645','A12140',\n",
    "        'A06252']\n",
    "getNotes = False\n",
    "outputfolder = 'playsTXT'\n",
    "convert(tcpIDs,getNotes,outputfolder)\n",
    "print(underscores)\n",
    "convert(underscores,getNotes,outputfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A21218 is not in EP\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Extract a particular English section from a TCP document with many languages and/or works \n",
    "'''\n",
    "outputfolder = 'playPartsTXT'\n",
    "folder = f'/Users/amycweng/Digital Humanities/{outputfolder}'\n",
    "tcpID = 'A21218' # Bilingual version of Ortho-epia Gallica\n",
    "# Want John Eliot's Parlement of Pratlers \n",
    "# Can only find the conclusion in English under a HEAD tag \n",
    "\n",
    "''' \n",
    "NOTE: Once you have found your target section under a tag in a TCP xml file, \n",
    "move the </HEAD> concluding tag to the bottom of the section. \n",
    "That way, only the target section has the associated text, not just the heading title \n",
    "\n",
    "Outputs the body text of a particular section to a TXT file. \n",
    "Names each TXT file by the {tcpID}_{section heading}\n",
    "'''\n",
    "def partialTextTCP(soup,target):\n",
    "    text_list = []\n",
    "    headings = soup.find_all('head')\n",
    "    for tag in headings:\n",
    "        children = tag.children\n",
    "        if target in tag.text: \n",
    "            for child in children:\n",
    "                text_list.append(child.text.strip())\n",
    "    return ' '.join(text_list[1:])\n",
    "\n",
    "head = 'The Conclusion of the Parlement of Pratlers.'\n",
    "path,source = '/Users/amycweng/Digital Humanities/editedA21218.P4.xml','TCP'\n",
    "with open(path,'r') as file: \n",
    "    data = file.read()\n",
    "targetTag = SoupStrainer(\"div3\",attrs={\"lang\": \"eng\"})\n",
    "soup = BeautifulSoup(data,parse_only=targetTag,features='html.parser')\n",
    "\n",
    "if source == 'TCP': \n",
    "    print(f'{tcpID} is not in EP')\n",
    "    bodytext = partialTextTCP(soup,head).lower()\n",
    "\n",
    "with open(f'{folder}/{tcpID}_{head}.txt', 'w+') as file:\n",
    "    bodytext = replaceTextLemma(bodytext,lemmaDict)\n",
    "    cleaned = cleanText(bodytext)\n",
    "    cleaned = cleaned.replace('\\n',' ')\n",
    "    file.write(f'{cleaned}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Extract a particular play from an EP xml version of an anthology \n",
    "\n",
    "The trick is to manually identify the section of the XML\n",
    "and then add the <play>... </play> tag to the beginning and end of the section. \n",
    "'''\n",
    "outputfolder = 'playPartsTXT'\n",
    "folder = f'/Users/amycweng/Digital Humanities/{outputfolder}'\n",
    "tcpID = 'A53060' # Margaret Cavendish's The Female Academy \n",
    "head = 'the female academy'\n",
    "path = '/Users/amycweng/Digital Humanities/editedA53060.xml'\n",
    "with open(path,'r') as file: \n",
    "    data = file.read()\n",
    "targetTag = SoupStrainer(\"play\")\n",
    "soup = BeautifulSoup(data,parse_only=targetTag,features='html.parser')\n",
    "bodytext = textEP(soup).lower()\n",
    "with open(f'{folder}/{tcpID}_{head}.txt', 'w+') as file:\n",
    "    bodytext = replaceTextLemma(bodytext,lemmaDict)\n",
    "    cleaned = cleanText(bodytext)\n",
    "    cleaned = cleaned.replace('\\n',' ')\n",
    "    file.write(f'{cleaned}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Separately extract each act of a play as a TXT file. \n",
    "\n",
    "NOTE: For EP XML files that contain only one play  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Separately extract each act of a play as a TXT file. \n",
    "\n",
    "NOTE: For EP XML files that are anthologies of plays \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
