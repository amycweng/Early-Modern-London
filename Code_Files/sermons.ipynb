{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from extractFeatures import getTexts\n",
    "\n",
    "csv_data = pd.read_csv('/Users/amycweng/Digital Humanities/Early-Modern-London/Sermons_Info/sermons.csv')\n",
    "tcpIDs = [ _ for _ in csv_data['id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the entire body text from an XML document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getTexts import *\n",
    "# EP: /Users/amycweng/Digital Humanities/eebotcp\n",
    "# TCP: /Users/amycweng/Digital Humanities/TCP\n",
    "outputfolder = 'charityTXT'\n",
    "convert(tcpIDs,outputfolder)\n",
    "print(underscores)\n",
    "convert(underscores,outputfolder)\n",
    "\n",
    "'''\n",
    "TCP charity texts that are not in EP: \n",
    "'B33867', 'B13871', 'B13868', 'B13878', 'B13862', 'B13860', \n",
    "'B09696', 'A86972', 'A19579', 'A09957'\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the sermons among the charity texts from their text files into a dictionary. Stopwords are removed in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "sermons = getTexts('/Users/amycweng/Digital Humanities/charityTXT',tcpIDs,True)\n",
    "print(len(sermons))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the context windows of 20 words around key terms into an output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {0: 'charity|charitable|charitie', \n",
    "        1: 'beneficience|bequeath|bestow|liberality|benevolence|goodwill| alm | almes |kindness|almsdeed',\n",
    "        2: 'poor|poverty|rich|wealthy|wealth|beggar|orphan|needy|labourer|impotent|bankrup|penury|meanness|prosperity|bounty|impouerish|impoverish',\n",
    "        3: 'rate|usury|usurer|money|commodity|talon|talent|coin|bullion|exchange|shilling|farthing| pound |penny|purchase',\n",
    "        4: 'credit|creditor|loan|lend',\n",
    "        5: ' debt|debtor|borrow|pledge|pawn',\n",
    "        6: 'industry|industrious|thrift|labor|labour|occupation|profit|profitable|commodious',\n",
    "        7: 'prison|imprisonment',\n",
    "        8: 'hospital|almshouse|orphanage',\n",
    "        9: 'city|citizen|london',\n",
    "        10: 'vagrant|vagrancy'\n",
    "        }\n",
    "search = [part for part in groups.values()]\n",
    "search = '|'.join(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/Users/amycweng/Digital Humanities/charityContexts'\n",
    "for tcpID,text in sermons.items(): \n",
    "    contexts = []\n",
    "    text = text.split(' ')\n",
    "    indices = []\n",
    "    for idx, word in enumerate(text):\n",
    "        toContinue = False\n",
    "        if re.match(search,word):  \n",
    "            for i in indices:\n",
    "                if idx in range(i-10,i+10):\n",
    "                    toContinue = True\n",
    "            if not toContinue: \n",
    "                indices.append(idx) \n",
    "                contexts.append(' '.join(text[idx-10:idx+10]))\n",
    "    with open(f'{outdir}/{tcpID}.txt','w+') as file: \n",
    "        file.write(' '.join(contexts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for sermons that have information about preaching location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns1 = [\"preach't (.*?)$\",\"preached at (.*?)$\",\"preached in (.*?)$\",\"preacht (.*?)$\",\"before the (.*?)$\", \"before his (.*?)$\"]\n",
    "patterns2 = [\"preacher at (.*?)$\",\"preacher in (.*?)$\",\"pastor at (.*?)$\",\"pastor of (.*?)$\"]\n",
    "places = []\n",
    "pastorPlaces = []\n",
    "import csv\n",
    "\n",
    "outFile = open(f'sermons_locations.csv','a+')\n",
    "columns = ['id','title','author','date','location']\n",
    "writer = csv.DictWriter(outFile, fieldnames=columns)\n",
    "writer.writeheader()\n",
    "\n",
    "located = []\n",
    "for idx, title in enumerate(csv_data['title']):\n",
    "    foundPattern1 = False\n",
    "    for pattern in patterns1: \n",
    "        place = re.search(rf'{pattern}', title)\n",
    "        if place: \n",
    "            # places.append(place.group())\n",
    "            dict = {'id': tcpIDs[idx], \n",
    "                    'title': title,\n",
    "                    'author': csv_data['author'][idx],\n",
    "                    'date': csv_data['date'][idx],\n",
    "                    'location': place.group()\n",
    "                    }\n",
    "            located.append(tcpIDs[idx])\n",
    "            writer.writerow(dict)\n",
    "            # print(f'{tcpIDs[idx]}: {place.group()}')\n",
    "            foundPattern1 = True\n",
    "            break\n",
    "    if not foundPattern1: \n",
    "        for pattern in patterns2: \n",
    "            place = re.search(rf'{pattern}', title)\n",
    "            if place: \n",
    "                dict = {'id': tcpIDs[idx], \n",
    "                    'title': title,\n",
    "                    'author': csv_data['author'][idx],\n",
    "                    'date': csv_data['date'][idx],\n",
    "                    'location': place.group()\n",
    "                }\n",
    "                # pastorPlaces.append(place.group())\n",
    "                located.append(tcpIDs[idx])\n",
    "                writer.writerow(dict)\n",
    "                foundPattern2 = True\n",
    "                break\n",
    "\n",
    "for idx, tcpID in enumerate(tcpIDs):\n",
    "    if tcpID in located: \n",
    "        continue\n",
    "    dict = {'id': tcpID, \n",
    "            'title': csv_data['title'][idx],\n",
    "            'author': csv_data['author'][idx],\n",
    "            'date': csv_data['date'][idx],\n",
    "            'location': ''\n",
    "            }\n",
    "    writer.writerow(dict)\n",
    "\n",
    "# print(len(places), len(pastorPlaces), len(places)+len(pastorPlaces))\n",
    "# print(places)\n",
    "# print(pastorPlaces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting title pages directly from TCP. \n",
    "Title pages often contain information about where the sermon was preached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "folder = f'/Users/amycweng/Digital Humanities/'\n",
    "outfile = open(f'{folder}/sermon_titles.csv','a+')\n",
    "columns = ['id','title page']\n",
    "writer = csv.DictWriter(outfile, fieldnames=columns)\n",
    "writer.writeheader()\n",
    "\n",
    "tcpIDs = ['A65610','A15015']\n",
    "for tcpID in tcpIDs: \n",
    "    path = findTextTCP(tcpID)\n",
    "    with open(path,'r') as file: \n",
    "        data = file.read()\n",
    "    tag = SoupStrainer(\"div1\", attrs={'type':'title page'})\n",
    "    soup = BeautifulSoup(data,parse_only=tag,features='html.parser')\n",
    "    title = []\n",
    "    sentences = soup.find_all('p')\n",
    "    for sentence in sentences: \n",
    "        title.append(sentence.text)\n",
    "    if len(title) != 0: \n",
    "        title = ' '.join(title)\n",
    "        title = re.sub('\\n',' ',title)\n",
    "        t = re.search(r'^(.*?) \\d{4}',title)\n",
    "        if t: \n",
    "            writer.writerow({'id': tcpID,  'title page': t.group()})\n",
    "        else: \n",
    "            writer.writerow({'id': tcpID,  'title page': title})\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
