{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gensim.models import Phrases,phrases\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.scripts import word2vec2tensor\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/amycweng/Digital Humanities/charityTXT'\n",
    "csv_data = pd.read_csv('/Users/amycweng/Digital Humanities/sermons.csv')\n",
    "tcpIDs = [ _ for _ in csv_data['id']]\n",
    "\n",
    "def getTexts(folder,search):\n",
    "    texts = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if 'NOTES' not in file: \n",
    "            name = file.split('.')[0]\n",
    "            if name in search: \n",
    "                path = os.path.join(folder,file)\n",
    "                with open(path,'r') as file: \n",
    "                    data = file.readlines()\n",
    "                if len(data) != 0: \n",
    "                    texts[name] = data[0]\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "all_texts = getTexts(folder,tcpIDs)\n",
    "tokenized = []\n",
    "for text in all_texts.values():\n",
    "    words = []\n",
    "    text = text.split(' ')\n",
    "    for t in text:\n",
    "        if len(t) != 0: words.append(t)\n",
    "    tokenized.append(words)\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creared word2vec model and store it\n",
    "word2vec = Word2Vec(tokenized, min_count=2,sg=1)\n",
    "# word2vec.wv.save_word2vec_format('/srv/data/tensor/sermons.model')\n",
    "word2vec.save('/Users/amycweng/Digital Humanities/Early-Modern-London/Other_Files/charity.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city: [('host', 0.8227683305740356), ('court', 0.8030896186828613), ('wilderness', 0.7994054555892944), ('palace', 0.7792544960975647), ('tabernacle', 0.778728723526001)]\n",
      "\n",
      "london: [('yawn', 0.9597653150558472), ('sodomitical', 0.9581200480461121), ('ioseph', 0.9568988084793091), ('defiance', 0.956531286239624), ('abiram', 0.9560275673866272)]\n",
      "\n",
      "citizen: [('faile', 0.939431369304657), ('auditor', 0.9220853447914124), ('attention', 0.9220227599143982), ('progenitor', 0.9193885326385498), ('tractable', 0.9163256287574768)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['charity','charitable','alm','bequest',\n",
    "        'poor','rate','usury','usurer','give','bequeath',\n",
    "        'help','money','beneficence','poverty','credit',\n",
    "        'creditor','loan','lend','new','novel','novelty',\n",
    "        'wealth','profit','profitable','beneficial',\n",
    "        'commodity','thrift','thrifty','industry']\n",
    "    \n",
    "words = ['city','london','citizen']\n",
    "\n",
    "for term in words: \n",
    "    if term in word2vec.wv: print(f'{term}: {word2vec.wv.most_similar(term,topn=5)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sermons relief: [('recovery', 0.8479760885238647), ('absence', 0.8261793255805969), ('succour', 0.8185170292854309), ('needy', 0.8065277934074402), ('solitariness', 0.8048096299171448), ('deprive', 0.8020899891853333), ('penury', 0.801383376121521), ('relieve', 0.8009129166603088), ('weal', 0.7992798089981079), ('pinch', 0.7955381274223328)]\n",
      "\n",
      "plays relief: [('sadness', 0.9876260757446289), ('accompany', 0.9873906970024109), ('amend', 0.9870021343231201), ('diligent', 0.9859092235565186), ('dispense', 0.985712468624115), ('allegiance', 0.9856859445571899), ('eyesight', 0.9850242137908936), ('lure', 0.9845280647277832), ('sue', 0.9837411642074585), ('provident', 0.9833546280860901)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['relief']\n",
    "sermons_model = Word2Vec.load('/Users/amycweng/Digital Humanities/Early-Modern-London/Other_Files/charity.model')\n",
    "play_model = Word2Vec.load('/Users/amycweng/Digital Humanities/Early-Modern-London/Other_Files/plays.model')\n",
    "for term in words: \n",
    "    if term in sermons_model.wv: print(f'sermons {term}: {sermons_model.wv.most_similar(term,topn=10)}\\n')\n",
    "    if term in play_model.wv: print(f'plays {term}: {play_model.wv.most_similar(term,topn=10)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PCA table\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(word2vec.wv.get_normed_vectors())\n",
    "pca_df = pd.DataFrame(pca_results, index=word2vec.wv.key_to_index, columns=[\"pc1\",\"pc2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA plots\n",
    "pca_df.plot(x='pc1',y='pc2',kind=\"scatter\",figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tsv file for word2vec model\n",
    "word2vec2tensor.word2vec2tensor('/srv/data/tensor/relevantep.model', '/srv/data/tensor/TSV/relevantep', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with label\n",
    "ax = pca_df.plot(x='pc1',y='pc2',kind=\"scatter\",figsize=(15, 10),alpha=0)\n",
    "for txt in pca_df.index:\n",
    "    if txt in ['charity','charities','talent','talents','profit','profits']:\n",
    "        x = pca_df.pc1.loc[txt]\n",
    "        y = pca_df.pc2.loc[txt]\n",
    "        ax.annotate(txt, (x,y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "\n",
    "def cosine_similarity(pair):\n",
    "\n",
    "    '''\n",
    "    Word2Vec.wv.similarity(word1,word2)\n",
    "    '''\n",
    "    word1, word2 = pair\n",
    "    if word1 and word2 in word2vec.wv:\n",
    "        sim = word2vec.wv.similarity(word1,word2)\n",
    "        return sim\n",
    "    return 0\n",
    "\n",
    "\n",
    "def generate_heatmap_matrix(lexicon):\n",
    "\n",
    "    pairs = list(combinations(lexicon, 2))\n",
    "    sim_scores = [cosine_similarity(pair) for pair in pairs if cosine_similarity(pair) != 0]\n",
    "\n",
    "    sim_df = pd.DataFrame({'pair': pairs, 'similarity': sim_scores})\n",
    "\n",
    "    sim_df['word1'] = sim_df['pair'].apply(lambda x: lexicon.index(x[0]))\n",
    "    sim_df['word2'] = sim_df['pair'].apply(lambda x: lexicon.index(x[1]))\n",
    "\n",
    "    sim_df['pair'] = list(zip(sim_df.word1, sim_df.word2, sim_df.similarity))\n",
    "\n",
    "    df_hm = pd.DataFrame({'word1': range(len(lexicon)),\n",
    "                          'word2': range(len(lexicon)),\n",
    "                          'similarity': pd.Series(np.ones(len(lexicon)))})\n",
    "\n",
    "    df_hm = df_hm.pivot(index='word1', columns='word2').fillna(0)\n",
    "\n",
    "    for row, col, similarity in sim_df.pair:\n",
    "\n",
    "        df_hm.iloc[col,row] = similarity\n",
    "\n",
    "    return df_hm\n",
    "\n",
    "\n",
    "def plot_heatmap(lexicon,title):\n",
    "\n",
    "    df_hm = generate_heatmap_matrix(lexicon)\n",
    "\n",
    "    mask = np.zeros_like(df_hm)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df_hm,\n",
    "                mask = mask,\n",
    "                xticklabels = lexicon,\n",
    "                yticklabels = lexicon,\n",
    "                cmap=\"YlGnBu\",\n",
    "                )\n",
    "    plt.xlabel('word 1')\n",
    "    plt.ylabel('word 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['charity','charitable','alm','bequest',\n",
    "        'poor','rate','usury','usurer','give','bequeath',\n",
    "        'help','money','beneficence','poverty','credit',\n",
    "        'creditor','loan','lend','new','novel','novelty',\n",
    "        'wealth','profit','profitable','beneficial',\n",
    "        'commodity','thrift','thrifty','industry']\n",
    "\n",
    "plot_heatmap(words,'Similarity Between Key Word-Pairs in Charity Texts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
